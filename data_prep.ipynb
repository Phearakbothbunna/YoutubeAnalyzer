{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T20:11:07.371615Z",
     "start_time": "2024-10-13T20:11:07.361121Z"
    }
   },
   "source": [
    "# Import needed libraries\n",
    "import pandas as pd\n",
    "\n",
    "### This is for loading the data\n",
    "\n",
    "# The crawl dataset we are using do not have column names or headings so we need to handle that\n",
    "column_names = ['videoID', 'uploader', 'age', 'category', 'length', 'views', 'rate', 'ratings', 'comments', 'relatedIDs']\n",
    "\n",
    "# Load YouTube data for each depth file(we have 4 depths in total starting from 0)\n",
    "def load_depth(path, crawl_date, depth):\n",
    "    # The separator of the dataset we are using is a tab-separated\n",
    "    depth_data = pd.read_csv(path, sep='\\t', header = None, on_bad_lines = 'skip', low_memory = False)\n",
    "    # Add the crawl date and depth number columns\n",
    "    depth_data['crawl_date'] = crawl_date\n",
    "    depth_data['depth'] = depth\n",
    "    return depth_data\n",
    "\n",
    "# Load all depth files for a single crawl\n",
    "def load_crawl(path, crawl_date):\n",
    "    # Load depth file 0, 1, 2 and 3\n",
    "    depth_files = [f\"{path}/{i}.txt\" for i in range(4)]\n",
    "    depth_dframe = []\n",
    "    # Loop through the total depth files and add the dataframe to depth_dframe list\n",
    "    for i, depth_file in enumerate (depth_files):\n",
    "        depth_dframe.append(load_depth(depth_file, crawl_date, i))\n",
    "    # Now for each crawl, we combine all depths into a single dataframe\n",
    "    combined_data = pd.concat(depth_dframe, axis=0).reset_index(drop = True)\n",
    "    return combined_data"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:11:11.683546Z",
     "start_time": "2024-10-13T20:11:11.651567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing\n",
    "df = load_depth(\"data/080327/0.txt\", '2008-03-27', \"0\")\n",
    "print(df)\n",
    "# df = load_crawl(\"data/080327\", \"2008-03-27\")\n",
    "# print(df.head())"
   ],
   "id": "5710194dee582db7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0                  1       2                 3      4  \\\n",
      "0    gFa1YMEJFag            sxephil  1135.0     Entertainment  270.0   \n",
      "1    pSJ4hv28zaI  thecomputernerd01  1136.0            Comedy  216.0   \n",
      "2    uHVEDq6RVXc    barelypolitical  1134.0   News & Politics   56.0   \n",
      "3    K7Om0QZy-38          SouljaBoy  1134.0             Music  185.0   \n",
      "4    DCAO6bZa31o    AssociatedPress  1134.0   News & Politics   45.0   \n",
      "..           ...                ...     ...               ...    ...   \n",
      "249  TLxiK5K2A7w       bosnaqiirlx3  1136.0            Comedy  252.0   \n",
      "250  hyswDZpNar8           beki0555  1136.0    Pets & Animals   24.0   \n",
      "251  fAffvIg2w1U       wavesg00dbye  1136.0             Music   34.0   \n",
      "252  sUYz2--i_Hw             ice500  1136.0     Entertainment   78.0   \n",
      "253  91gEbvB9U_M         corentin51  1136.0  Autos & Vehicles    4.0   \n",
      "\n",
      "            5     6       7       8            9  ...           21  \\\n",
      "0    101384.0  4.72  3407.0  2887.0  QuRYeRnAuXM  ...  I4yKEK9o8gA   \n",
      "1       458.0  4.80   133.0  2183.0  dh6dF1XY3uI  ...  QJMZjx4L0BA   \n",
      "2    555203.0  4.70  3574.0  2117.0  aYHBqH_xbCw  ...  bjAuzPnJ82A   \n",
      "3     91293.0  3.19  1063.0  1132.0  UCeA4K2-wNk  ...  Cj_S_rut3LQ   \n",
      "4    108095.0  3.58   264.0  1069.0  5vLbA7n8EG0  ...  16s-pJs69UU   \n",
      "..        ...   ...     ...     ...          ...  ...          ...   \n",
      "249       0.0  0.00     0.0     0.0  P7eR8CWEDkU  ...  hxYB6D3bvAA   \n",
      "250       0.0  0.00     0.0     0.0  ZCYaw5tGYAs  ...  Lln5i1N3J8g   \n",
      "251       0.0  0.00     0.0     0.0          NaN  ...          NaN   \n",
      "252       5.0  0.00     0.0     0.0  kPrIkTwoStY  ...  buHrlMJu2Ss   \n",
      "253       0.0  0.00     0.0     0.0          NaN  ...          NaN   \n",
      "\n",
      "              22           23           24           25           26  \\\n",
      "0    1GKaVzNDbuI  yuZhwV24PmM  zQ83d_D2MGs  DomumdGQSg8  pFUYi7dp1WU   \n",
      "1    OshpuI9BGVU  eBGIQ7ZuuiU  L3mR8syHNIg  McZ3lP8lb6E  WsaQ0dQhXnA   \n",
      "2    E8_odhuWfTk  TKe2rBbGGEA  lwrxclIGJqk  T3depGF5E-0  9l2qbaOMbcU   \n",
      "3    XNyINFMHHm4  6hIis-_GIXk  hd6yq-HIibI  ZEDYzwdqHrU  t02kRsw1AUU   \n",
      "4    kHKefMuCqlk  1A1thQqUvAI  awVJCNMw1To  FJETon8XRxY  3t8GdtYdRk0   \n",
      "..           ...          ...          ...          ...          ...   \n",
      "249  b-Tbc0dyq1Q  u0Qq88DIFHc  LcVfoXOugdI  Ie_u-QNifmk  mE9dlVrkSFE   \n",
      "250  JnsOORuRB-M  We8P_Ww27hY  DGQVX8iGbgk  TnxQDrdIV2Q  E4VMntSUskg   \n",
      "251          NaN          NaN          NaN          NaN          NaN   \n",
      "252  S95602a7cFE  esCvu46qDaA  kOZ8bRAO7YQ  GSsVZcKHues  foKRqJGcbPQ   \n",
      "253          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "              27           28  crawl_date depth  \n",
      "0    2l6vwAIAqNU  hiSmlmXp-aU  2008-03-27     0  \n",
      "1    16XHS2R_mEo  lhGZVgoDub0  2008-03-27     0  \n",
      "2    8BfNqhV5hg4  2hZx3yBoNw4  2008-03-27     0  \n",
      "3    H4kZqk1NF4U  J52gclijmWw  2008-03-27     0  \n",
      "4    o5t3PwKccOo  ype6Hw_Hmjo  2008-03-27     0  \n",
      "..           ...          ...         ...   ...  \n",
      "249  bNEdkACBNR8  Q6fAVMacYnk  2008-03-27     0  \n",
      "250  GJ66UO-iKpc  crmD_B8ERzk  2008-03-27     0  \n",
      "251          NaN          NaN  2008-03-27     0  \n",
      "252  -CYhSNHbeC8  HlF6fbIFiCM  2008-03-27     0  \n",
      "253          NaN          NaN  2008-03-27     0  \n",
      "\n",
      "[254 rows x 31 columns]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:11:40.302818Z",
     "start_time": "2024-10-13T20:11:40.295507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### This is for cleaning the data\n",
    "# def clean_data(df):\n",
    "#     # We remove any leading/trailing whitespace from string columns\n",
    "#     df['uploader'] = df['uploader'].str.strip()\n",
    "#     df['category'] = df['category'].str.strip()\n",
    "    # Convert 'age' to integer\n",
    "    \n",
    "# Clean the data\n",
    "def clean_data(df):\n",
    "    # Since there may be more than 1 related ids, we handle it by combining the related IDs together as a single list\n",
    "    combined_related_ids = []\n",
    "    # Loop over each row and combine the related IDs\n",
    "    for index, row in df.iterrows():\n",
    "        # We select the related IDs columns (from the 10th column onward)\n",
    "        related_ids = row[9:]  \n",
    "        # Join the cleaned related IDs into a single string separated by commas then add to the list\n",
    "        combined_related_ids.append(','.join(related_ids.astype(str)))\n",
    "    \n",
    "    # Add the combined relatedIDs to the DataFrame\n",
    "    df['relatedIDs'] = combined_related_ids\n",
    "    # Keep only the first 9 columns plus the new combined 'relatedIDs' column\n",
    "    depth_data = df.iloc[:, :9].copy() \n",
    "    depth_data['relatedIDs'] = combined_related_ids \n",
    "    # Add the column names to each column\n",
    "    depth_data.columns = column_names\n",
    "    \n",
    "    # Remove leading/trailing whitespace from string columns\n",
    "    depth_data['uploader'] = depth_data['uploader'].str.strip()\n",
    "    depth_data['category'] = depth_data['category'].str.strip()\n",
    "\n",
    "    # Convert 'age' to integer (handle errors with 'coerce')\n",
    "    depth_data['age'] = pd.to_numeric(depth_data['age'], errors='coerce')\n",
    "\n",
    "    # Fill missing 'rate' values with the mean of the column\n",
    "    depth_data['rate'] = depth_data['rate'].fillna(depth_data['rate'].mean())\n",
    "    \n",
    "    \n",
    "    # 4. Convert 'related_ids' to list format (split by whitespace)\n",
    "    # df['related_ids'] = df['related_ids'].str.split()\n",
    "\n",
    "    return depth_data\n",
    "\n",
    "\n",
    "# # Transform the data\n",
    "# def transform_data(df):\n",
    "#     # 1. Normalize the 'length' column to ensure it's integer\n",
    "#     df['length'] = df['length'].astype(int)\n",
    "# \n",
    "#     # 2. Create a new column 'views_per_rating' to avoid division by zero\n",
    "#     df['views_per_rating'] = df['views'] / (df['ratings'] + 1)\n",
    "# \n",
    "#     # 3. Convert 'category' to categorical type for efficiency\n",
    "#     df['category'] = df['category'].astype('category')\n",
    "# \n",
    "#     return df"
   ],
   "id": "b16f2c66de2967c8",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Function to connect and insert into MongoDB\n",
    "# def insert_to_mongo(db, collection_name, records):\n",
    "#     # Insert records into MongoDB\n",
    "#     db[collection_name].insert_many(records)\n",
    "# \n",
    "# # Main function\n",
    "# def main():\n",
    "#     # For each crawl, load and process all depth files\n",
    "#     crawl1 = load_crawl('data/080327', '2008-03-27')\n",
    "#     crawl2 = load_crawl('data/080329', '2008-03-29')\n",
    "#     crawl3 = load_crawl('data/080331', '2008-03-31')\n",
    "# \n",
    "#     # Combine all crawls into a single dataframe\n",
    "#     combined_data = pd.concat([crawl1, crawl2, crawl3], axis=0).reset_index(drop=True)\n",
    "# \n",
    "#     # Clean the combined data\n",
    "#     combined_data = clean_data(combined_data)\n",
    "# \n",
    "#     # Transform the combined data\n",
    "#     combined_data = transform_data(combined_data)\n",
    "# \n",
    "#     print(combined_data.head())\n",
    "# main()"
   ],
   "id": "b548994c9f0af3e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:13:01.755839Z",
     "start_time": "2024-10-13T20:13:01.707603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "testing = clean_data(load_depth(\"data/080327/0.txt\", '2008-03-27', \"0\"))\n",
    "print(testing.head())"
   ],
   "id": "185e68b1be917648",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       videoID           uploader     age         category  length     views  \\\n",
      "0  gFa1YMEJFag            sxephil  1135.0    Entertainment   270.0  101384.0   \n",
      "1  pSJ4hv28zaI  thecomputernerd01  1136.0           Comedy   216.0     458.0   \n",
      "2  uHVEDq6RVXc    barelypolitical  1134.0  News & Politics    56.0  555203.0   \n",
      "3  K7Om0QZy-38          SouljaBoy  1134.0            Music   185.0   91293.0   \n",
      "4  DCAO6bZa31o    AssociatedPress  1134.0  News & Politics    45.0  108095.0   \n",
      "\n",
      "   rate  ratings  comments                                         relatedIDs  \n",
      "0  4.72   3407.0    2887.0  QuRYeRnAuXM,3TYqkBJ9YRk,rSJ8QZWBegU,nRcovJn9xH...  \n",
      "1  4.80    133.0    2183.0  dh6dF1XY3uI,_a0gQFOJYWM,UzPldH0vuHY,h9gRdAmGFn...  \n",
      "2  4.70   3574.0    2117.0  aYHBqH_xbCw,SfaxA9Q-9AQ,1cWWE3A2mDI,exT_E9FNu8...  \n",
      "3  3.19   1063.0    1132.0  UCeA4K2-wNk,BDmhe0vIFiQ,9xSvVPa41Cg,3Cc7-4OeAg...  \n",
      "4  3.58    264.0    1069.0  5vLbA7n8EG0,3ZbXJp-NUZc,McYsnvAymV8,MaE1kowuCB...  \n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
