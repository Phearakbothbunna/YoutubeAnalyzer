{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "# Import needed libraries\n",
    "import pandas as pd\n",
    "\n",
    "### This is for loading the data\n",
    "\n",
    "# The crawl dataset we are using do not have column names or headings so we need to handle that\n",
    "column_names = ['videoID', 'uploader', 'age', 'category', 'length', 'views', 'rate', 'ratings', 'comments', 'relatedIDs']\n",
    "\n",
    "# Load YouTube data for each depth file(we have 4 depths in total starting from 0)\n",
    "def load_depth(path, crawl_date, depth):\n",
    "    # Initialize a list to store the valid rows\n",
    "    valid_rows = []\n",
    "    # Read our dataset line by line\n",
    "    # We were using pandas to load/read data at first but there were errors when the 1st row only has 1 or 2 column and pandas were assuming that's all the columns we have (inconsistent)\n",
    "    # so we decided to go with this route instead and it works\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split the line by tab and check if it has at least 2 columns\n",
    "            # Some rows might have only 1 or 2 columns which is not useful for us, so we ignore them\n",
    "            # this way it helps with fixing the issues with some rows having only 1 column as well\n",
    "            split_line = line.strip().split('\\t')\n",
    "            if len(split_line) >= 2:\n",
    "                valid_rows.append(split_line)       \n",
    "    # Convert the list into pandas dataframe\n",
    "    depth_data = pd.DataFrame(valid_rows)\n",
    "    depth_data['crawl_date'] = crawl_date\n",
    "    depth_data['depth'] = depth\n",
    "    return depth_data\n",
    "\n",
    "# Load all depth files for a single crawl\n",
    "def load_crawl(path, crawl_date):\n",
    "    # Load depth file 0, 1, 2 and 3\n",
    "    depth_files = [f\"{path}/{i}.txt\" for i in range(4)]\n",
    "    depth_dframe = []\n",
    "    # Loop through the total depth files and add the dataframe to depth_dframe list\n",
    "    for i, depth_file in enumerate (depth_files):\n",
    "        depth_dframe.append(load_depth(depth_file, crawl_date, i))\n",
    "    # Now for each crawl, we combine all depths into a single dataframe\n",
    "    combined_data = pd.concat(depth_dframe, axis=0).reset_index(drop = True)\n",
    "    return combined_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b16f2c66de2967c8",
   "metadata": {},
   "source": [
    "### This is for cleaning and transformation the data (Data Preparation)\n",
    "def prepare_data(df):\n",
    "    # Since there may be more than 1 related ids, we handle it by combining the related IDs together as a single list\n",
    "    combined_related_ids = []\n",
    "    # Loop over each row and combine the related IDs\n",
    "    for index, row in df.iterrows():\n",
    "        # We select the related IDs columns (from the 10th column onward) \n",
    "        # The dataset description says the related IDs is up to 20 strings only \n",
    "        related_ids = row[9:29]  \n",
    "        # Join the cleaned related IDs into a single string separated by commas then add to the list\n",
    "        combined_related_ids.append(','.join(related_ids.astype(str)))\n",
    "    # Add the combined relatedIDs to the DataFrame\n",
    "    df['relatedIDs'] = combined_related_ids\n",
    "    # Keep only the first 9 columns plus the new combined 'relatedIDs' column\n",
    "    depth_data = df.iloc[:, :9].copy() \n",
    "    depth_data['relatedIDs'] = combined_related_ids \n",
    "    # Keep the crawl_date and depth column \n",
    "    depth_data[['crawl_date', 'depth']] = df[['crawl_date', 'depth']] \n",
    "    # Add the column names to each column\n",
    "    depth_data.columns = column_names + ['crawl_date', 'depth']\n",
    "    # Remove the leading/trailing whitespace from string columns\n",
    "    depth_data['uploader'] = depth_data['uploader'].str.strip()\n",
    "    depth_data['category'] = depth_data['category'].str.strip()\n",
    "    # Convert these columns to numeric\n",
    "    numeric_columns = ['age', 'length', 'views', 'rate', 'ratings', 'comments']\n",
    "    for col in numeric_columns:\n",
    "        depth_data[col] = pd.to_numeric(depth_data[col])   \n",
    "    # Fill in the missing 'rate' values with the mean of the column\n",
    "    depth_data['rate'] = depth_data['rate'].fillna(depth_data['rate'].mean())\n",
    "    return depth_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "185e68b1be917648",
   "metadata": {},
   "source": [
    "# testing = (load_depth(\"data/080331/2.txt\", '2008-03-31', \"2\"))\n",
    "# clean = clean_data(testing)\n",
    "# print(clean)\n",
    "\n",
    "# Load all the crawls in the given dataset\n",
    "crawl1 = load_crawl('data/080327', '2008-03-27')\n",
    "crawl2 = load_crawl('data/080329', '2008-03-29')\n",
    "crawl3 = load_crawl('data/080331', '2008-03-31')\n",
    "# Combine all crawls into a single dataframe\n",
    "combined_data = pd.concat([crawl1, crawl2, crawl3], axis=0).reset_index(drop=True)\n",
    "# Prepare the data (clean & transform)\n",
    "combined_data = prepare_data(combined_data)\n",
    "print(combined_data.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28e03963de74113b",
   "metadata": {},
   "source": [
    "# Data ingestion and connection with MongoDB\n",
    "from pymongo import MongoClient\n",
    "# Connect to MongoDb database that we have created\n",
    "uri = 'mongodb://localhost:27017/'\n",
    "client = MongoClient(uri)\n",
    "db = client[\"youtubedb\"]\n",
    "collection = db[\"youtube_vids\"]\n",
    "# this prevents duplicate from running this section more than once\n",
    "collection.delete_many({})\n",
    "# Convert the DataFrame into a list of dictionaries\n",
    "insert_data = combined_data.to_dict('records')\n",
    "# Insert the data into the collection\n",
    "collection.insert_many(insert_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1b431fb-5ac4-453b-a3e7-921701e09b87",
   "metadata": {},
   "source": [
    "# Validation Queries\n",
    "\n",
    "# Query 1: Validate Document Count\n",
    "document_count = collection.count_documents({})\n",
    "print(\"Total documents:\", document_count)\n",
    "\n",
    "# Query 2: Check Category Distribution\n",
    "category_counts = collection.aggregate([\n",
    "    { \"$group\": { \"_id\": \"$category\", \"count\": { \"$sum\": 1 } } }\n",
    "])\n",
    "print(\"Category distribution:\")\n",
    "for category in category_counts:\n",
    "    print(category)\n",
    "\n",
    "# Query 3: Check Average and Max Views\n",
    "view_stats = collection.aggregate([\n",
    "    { \"$group\": { \"_id\": None, \"avgViews\": { \"$avg\": \"$views\" }, \"maxViews\": { \"$max\": \"$views\" } } }\n",
    "])\n",
    "print(\"View statistics:\")\n",
    "for stats in view_stats:\n",
    "    print(stats)\n",
    "\n",
    "# Query 4: Validate Related IDs Format\n",
    "related_id_check = collection.find({\"relatedIDs\": {\"$exists\": True, \"$type\": \"string\"}}).limit(5)\n",
    "print(\"Sample related IDs:\")\n",
    "for doc in related_id_check:\n",
    "    print(doc['relatedIDs'])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4535ff3b-a8ed-4402-8d84-a42a18e194f0",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "# Clear the collection to avoid duplicate entries\n",
    "collection.delete_many({})\n",
    "print(\"Collection cleared.\")\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Ingest data (insert many records)\n",
    "collection.insert_many(insert_data)\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print ingestion time\n",
    "ingestion_time = end_time - start_time\n",
    "print(f\"Data ingestion took {ingestion_time:.2f} seconds\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0a863e2-6488-4d10-a662-9a607141701d",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "# Measure execution time for document count\n",
    "start_time = time.time()\n",
    "document_count = collection.count_documents({})\n",
    "end_time = time.time()\n",
    "\n",
    "# Print results\n",
    "print(\"Total documents:\", document_count)\n",
    "print(f\"Document count query took {end_time - start_time:.4f} seconds\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "158889be-f784-46e1-a43f-c11ff8a84031",
   "metadata": {},
   "source": [
    "# Measure execution time for category distribution query\n",
    "start_time = time.time()\n",
    "category_counts = collection.aggregate([\n",
    "    { \"$group\": { \"_id\": \"$category\", \"count\": { \"$sum\": 1 } } }\n",
    "])\n",
    "\n",
    "# Collect results to ensure the query executes fully\n",
    "categories = list(category_counts)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print results and time\n",
    "print(\"Category distribution:\")\n",
    "for category in categories:\n",
    "    print(category)\n",
    "print(f\"Category distribution query took {end_time - start_time:.4f} seconds\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a7e1f2b4-3f7a-458a-9cc2-e1041f50c426",
   "metadata": {},
   "source": [
    "# Measure execution time for view statistics query\n",
    "start_time = time.time()\n",
    "view_stats = collection.aggregate([\n",
    "    { \"$group\": { \"_id\": None, \"avgViews\": { \"$avg\": \"$views\" }, \"maxViews\": { \"$max\": \"$views\" } } }\n",
    "])\n",
    "\n",
    "# Collect results to ensure the query executes fully\n",
    "view_stats_result = list(view_stats)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print results and time\n",
    "print(\"View statistics:\", view_stats_result)\n",
    "print(f\"View statistics query took {end_time - start_time:.4f} seconds\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b586804-b789-4b02-abfb-be84f4444aa6",
   "metadata": {},
   "source": [
    "# Measure execution time for related IDs format check\n",
    "start_time = time.time()\n",
    "related_id_check = collection.find({\"relatedIDs\": {\"$exists\": True, \"$type\": \"string\"}}).limit(5)\n",
    "\n",
    "# Collect results to ensure the query executes fully\n",
    "related_ids = list(related_id_check)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print results and time\n",
    "print(\"Sample related IDs:\")\n",
    "for doc in related_ids:\n",
    "    print(doc['relatedIDs'])\n",
    "print(f\"Related IDs check query took {end_time - start_time:.4f} seconds\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f75f83204e0715aa",
   "metadata": {},
   "source": [
    "# Running aggregation functions (from aggregations.py) and displaying results\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from aggregations import (\n",
    "    calculate_view_statistics,\n",
    "    calculate_avg_rating_by_category,\n",
    "    calculate_total_comments_and_ratings_by_category,\n",
    "    calculate_avg_length_by_category,\n",
    "    avg_views_per_video_by_category,\n",
    "    most_viewed_videos,\n",
    "    most_viewed_video_in_each_category,\n",
    "    top_uploader_by_views,\n",
    "    top_commented_videos\n",
    ")\n",
    "\n",
    "# MongoDB connection setup\n",
    "uri = 'mongodb://localhost:27017/'\n",
    "client = MongoClient(uri)\n",
    "db = client[\"youtubedb\"]\n",
    "collection = db[\"youtube_vids\"]\n",
    "\n",
    "# 1. Platform-wide View Statistics\n",
    "view_stats = list(calculate_view_statistics(collection))[0]\n",
    "print(\"\\nPlatform-Wide View Statistics:\")\n",
    "print(f\"  - Average Views: {view_stats['avgViews']:.2f}\")\n",
    "print(f\"  - Maximum Views: {view_stats['maxViews']}\")\n",
    "print(f\"  - Minimum Views: {view_stats['minViews']}\")\n",
    "\n",
    "# 2. Average Rating by Category\n",
    "avg_rating_by_category = list(calculate_avg_rating_by_category(collection))\n",
    "rating_df = pd.DataFrame(avg_rating_by_category)\n",
    "rating_df.columns = [\"Category\", \"Average Rating\"]\n",
    "rating_df[\"Average Rating\"] = rating_df[\"Average Rating\"].round(2)\n",
    "print(\"\\nAverage Rating by Category:\")\n",
    "display(rating_df)\n",
    "\n",
    "# 3. Total Comments and Ratings by Category\n",
    "comments_ratings_by_category = list(calculate_total_comments_and_ratings_by_category(collection))\n",
    "comments_ratings_df = pd.DataFrame(comments_ratings_by_category)\n",
    "comments_ratings_df.columns = [\"Category\", \"Total Comments\", \"Total Ratings\"]\n",
    "print(\"\\nTotal Comments and Ratings by Category:\")\n",
    "display(comments_ratings_df)\n",
    "\n",
    "# 4. Average Video Length by Category\n",
    "avg_length_by_category = list(calculate_avg_length_by_category(collection))\n",
    "length_df = pd.DataFrame(avg_length_by_category)\n",
    "length_df.columns = [\"Category\", \"Average Length\"]\n",
    "print(\"\\nAverage Video Length by Category:\")\n",
    "display(length_df)\n",
    "\n",
    "# 5. Average Views per Video by Category\n",
    "avg_views_category = list(avg_views_per_video_by_category(collection))\n",
    "avg_views_df = pd.DataFrame(avg_views_category)\n",
    "avg_views_df.columns = [\"Category\", \"Average Views\"]\n",
    "print(\"\\nAverage Views per Video by Category:\")\n",
    "display(avg_views_df)\n",
    "\n",
    "# 6. Most Viewed Videos on the Platform\n",
    "most_viewed = list(most_viewed_videos(collection, top_n=5))\n",
    "most_viewed_df = pd.DataFrame(most_viewed)\n",
    "print(\"\\nTop 5 Most Viewed Videos on the Platform:\")\n",
    "display(most_viewed_df)\n",
    "\n",
    "# 7. Most Viewed Video in Each Category\n",
    "most_viewed_per_category = list(most_viewed_video_in_each_category(collection))\n",
    "most_viewed_category_df = pd.DataFrame(most_viewed_per_category)\n",
    "print(\"\\nMost Viewed Videos in Each Category:\")\n",
    "display(most_viewed_category_df)\n",
    "\n",
    "# 8. Top Uploader by Total Views\n",
    "top_uploader = list(top_uploader_by_views(collection))[0]\n",
    "print(\"\\nTop Uploader by Total Views:\")\n",
    "print(f\"  - Uploader: {top_uploader['_id']}\")\n",
    "print(f\"  - Total Views: {top_uploader['totalViews']}\")\n",
    "\n",
    "# 9. Top 5 Most Commented Videos\n",
    "top_commented = list(top_commented_videos(collection))\n",
    "top_commented_df = pd.DataFrame(top_commented)\n",
    "print(\"\\nTop 5 Most Commented Videos:\")\n",
    "display(top_commented_df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f0046bc1889abb8",
   "metadata": {},
   "source": [
    "from graphframes import GraphFrame\n",
    "from pyspark.sql.functions import col, explode, split\n",
    "from pyspark.sql import SparkSession\n",
    "# PageRank for influence analysis\n",
    "spark = ((((SparkSession.builder \n",
    "            .appName(\"YouTubeAnalyzer\"))\n",
    "            .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.4.0,\" \n",
    "                    \"graphframes:graphframes:0.8.4-spark3.5-s_2.12\")\n",
    "            .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/youtubedb.youtube_vids\"))\n",
    "            .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/youtubedb.pagerank_results\"))\n",
    "            # The configs below prevents out of memory error that our team ran into and also increase efficiency\n",
    "            .config(\"spark.driver.memory\", \"8g\")\n",
    "            .config(\"spark.executor.memory\", \"8g\")\n",
    "            .config(\"spark.executor.instances\", \"3\")\n",
    "            .config(\"spark.executor.cores\", \"7\")\n",
    "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "            .getOrCreate())\n",
    "# Read data from Mongodb\n",
    "yt_data = (((spark.read.format(\"mongodb\") \n",
    "           .option(\"database\", \"youtubedb\")) \n",
    "           .option(\"collection\", \"youtube_vids\"))\n",
    "           .load())\n",
    "\n",
    "# Check to see if the pagerank results already exist in the database\n",
    "pagerank_results = (spark.read.format(\"mongodb\")\n",
    "                    .option(\"database\", \"youtubedb\")\n",
    "                    .option(\"collection\", \"pagerank_results\")\n",
    "                    .load())\n",
    "# Just load the results and display without having to rerun the whole PageRank algorithm if it already exists in database\n",
    "if pagerank_results.count() > 0:\n",
    "    print(\"PageRank results already exist in Database. Loading results...\")\n",
    "    sample_results = pagerank_results.select(\"id\", \"uploader\", \"category\", \"pagerank\", \"comments\", \"ratings\")\n",
    "\n",
    "else:\n",
    "    # Repartition the DF into 128 partitions (for parallel processing)\n",
    "    # Our team decided to store repartitioned result in memory to prevent re-computation or reloading\n",
    "    # (we believe this can improve performance)\n",
    "    yt_data_full = yt_data.repartition(128).cache()\n",
    "    # Create dataframe for vertices (videos)\n",
    "    vertices = yt_data_full.select(\n",
    "        col(\"videoID\").alias(\"id\"),\n",
    "        col(\"uploader\"),\n",
    "        col(\"category\"),\n",
    "        col(\"views\"),\n",
    "        col(\"rate\"),\n",
    "        col(\"ratings\"),\n",
    "        col(\"comments\"),\n",
    "        col(\"depth\")\n",
    "    )\n",
    "    # Create dataframe for edges (related connections) and have a new column named dst\n",
    "    # Explode will create separate row for each related videoID in dst\n",
    "    edges = yt_data_full.select(\n",
    "        col(\"videoID\").alias(\"src\"),\n",
    "        split(col(\"relatedIDs\"), \",\").alias(\"dst\")\n",
    "    ).withColumn(\"dst\", explode(col(\"dst\")))\n",
    "    # Create GraphFrame\n",
    "    vid_graph = GraphFrame(vertices, edges)\n",
    "    # PageRank algorithm\n",
    "    # resetProbability controls how likely a random surfer will jump to a random page or node\n",
    "    # Since link structure is important, our team decided to use a low resetProbability\n",
    "    start_time = time.time()\n",
    "    influences = vid_graph.pageRank(resetProbability = 0.15, tol=0.05)\n",
    "    end_time = time.time()\n",
    "    # Show total time it takes to apply pagerank algorithm on the whole dataset\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time for applying PageRank on the full dataset: {execution_time}s\")\n",
    "    # Save the PageRank results back to MongoDB\n",
    "    (influences.vertices.write.format(\"mongodb\").mode(\"overwrite\")\n",
    "        .option(\"database\", \"youtubedb\")\n",
    "        .option(\"collection\", \"pagerank_results\").save())\n",
    "    # Display sample of the results\n",
    "    sample_results = influences.vertices.select(\"id\", \"uploader\", \"category\", \"pagerank\", \"comments\", \"ratings\")\n",
    "\n",
    "sample_results.show(1000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81c55cf201afb13c",
   "metadata": {},
   "source": [
    "# Get top 10 videos with the lowest PageRank scores\n",
    "lowest_pagerank_videos = (pagerank_results\n",
    "                              .select(\"id\", \"uploader\", \"category\", \"pagerank\", \"comments\", \"ratings\", \"views\")\n",
    "                              .orderBy(col(\"pagerank\").asc())\n",
    "                              .limit(10))\n",
    "print(\"Top 10 videos with the lowest PageRank scores:\")\n",
    "lowest_pagerank_videos.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bcc99ca3b810bc49",
   "metadata": {},
   "source": [
    "As we can see from the output above, the top 10 YouTube videos with the lowest PageRank scores all have similar or the same scores (around 0.34) and this is expected due to the lack of quality inbound links, comments, ratings and views. Moreover, our team uses tol = 0.05, which means that the algorithm stops iterating when the changes in PageRank values are below the threshold of 0.05, causing the low-scoring videos to converge to similar scores."
   ]
  },
  {
   "cell_type": "code",
   "id": "ca266c52d705eaca",
   "metadata": {},
   "source": [
    "# Get top 10 videos with highest PageRank scores\n",
    "highest_pagerank_videos = (pagerank_results\n",
    "                              .select(\"id\", \"uploader\", \"category\", \"pagerank\", \"comments\", \"ratings\", \"views\")\n",
    "                              .orderBy(col(\"pagerank\").desc())\n",
    "                              .limit(10))\n",
    "print(\"Top 10 videos with the highest PageRank scores:\")\n",
    "highest_pagerank_videos.show()\n",
    "# spark.stop()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23ed56ce",
   "metadata": {},
   "source": [
    "from pymongo import MongoClient\n",
    "from collections import Counter\n",
    "\n",
    "# client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "db = client[\"youtubedb\"]\n",
    "collection = db[\"youtube_vids\"]\n",
    "\n",
    "k = 10\n",
    "\n",
    "def top_k_categories(collection, k):\n",
    "    pipeline = [\n",
    "        {\"$group\": {\"_id\": \"$category\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}},\n",
    "        {\"$limit\": k}\n",
    "    ]\n",
    "\n",
    "    top_categories = list(collection.aggregate(pipeline))\n",
    "\n",
    "    print(\"Top categories:\")\n",
    "    for entry in top_categories:\n",
    "        print(f\"{entry['_id']}: {entry['count']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "top_k_categories(collection, k)\n",
    "\n",
    "# Top categories:\n",
    "# Entertainment 162044\n",
    "# Music 156208\n",
    "# Comedy 61113\n",
    "# People & Blogs 56036\n",
    "# Film & Animation 51459\n",
    "# Sports 48265\n",
    "# News & Politics 30913\n",
    "# Autos & Vehicles 19237\n",
    "# Howto & Style 18031\n",
    "# Pets & Animals 11325\n",
    "\n",
    "def top_k_categories_gui(collection, k):\n",
    "    pipeline = [\n",
    "        {\"$group\": {\"_id\": \"$category\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}},\n",
    "        {\"$limit\": k}\n",
    "    ]\n",
    "    top_categories = list(collection.aggregate(pipeline))\n",
    "    return top_categories"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "507293d0",
   "metadata": {},
   "source": [
    "#Top k rated videos\n",
    "def top_k_rated_videos():\n",
    "    print(\"Top rated videos:\")\n",
    "    top_rated_videos = collection.find().sort(\"rate\", -1).limit(k)\n",
    "    for video in top_rated_videos:\n",
    "        print(video[\"videoID\"], video[\"rate\"])\n",
    "    print(\"\\n\")\n",
    "\n",
    "top_k_rated_videos()\n",
    "\n",
    "def top_k_rated_videos_gui():\n",
    "    top_rated_videos = collection.find().sort(\"rate\", -1).limit(k)\n",
    "    return top_rated_videos"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "efab2439",
   "metadata": {},
   "source": [
    "#Top k viewed videos\n",
    "def top_k_viewed_videos(collection, k):\n",
    "    print(\"Top viewed videos:\")\n",
    "    top_viewed_videos = collection.find().sort(\"views\", -1).limit(k)\n",
    "    for video in top_viewed_videos:\n",
    "        print(video[\"videoID\"], video[\"views\"])\n",
    "    print(\"\\n\")\n",
    "\n",
    "top_k_viewed_videos(collection, k)\n",
    "\n",
    "def top_k_viewed_videos_gui(collection, k):\n",
    "    top_viewed_videos = collection.find().sort(\"views\", -1).limit(k)\n",
    "    return top_viewed_videos"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1bb8e356",
   "metadata": {},
   "source": [
    "#Range queries\n",
    "\n",
    "# Find videos in specific category with a duration between t1 and t2 seconds\n",
    "category = \"Comedy\" \n",
    "t1 = 700 \n",
    "t2 = 1000 \n",
    "\n",
    "def videos_in_length_range_by_category(category, t1, t2):\n",
    "    category_query = {\n",
    "    \"category\": category,\n",
    "    \"length\": {\"$gte\": t1, \"$lte\": t2}  # Range query for length\n",
    "}\n",
    "\n",
    "    print(f\"Videos in category {category} with duration between {t1} and {t2} seconds:\")\n",
    "    result = collection.find(category_query)\n",
    "    for video in result:\n",
    "        print(f\"Video ID: {video['videoID']}, Category: {video['category']}, Length: {video['length']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "videos_in_length_range_by_category(category, t1, t2)\n",
    "\n",
    "def videos_in_length_range_by_category_gui(collection, category, t1, t2):\n",
    "    category_query = {\n",
    "        \"category\": category,\n",
    "        \"length\": {\"$gte\": t1, \"$lte\": t2}\n",
    "    }\n",
    "    result = collection.find(category_query)\n",
    "    return list(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f6a53d6",
   "metadata": {},
   "source": [
    "t1 = 500\n",
    "t2 = 1000 \n",
    "def get_videos_in_length_range(t1, t2):\n",
    "    general_query = {\n",
    " \"length\": {\"$gte\": t1, \"$lte\": t2}\n",
    "}\n",
    "    print(f\"All videos with duration between {t1} and {t2} seconds:\")\n",
    "    result = collection.find(general_query)\n",
    "    for video in result:\n",
    "        print(f\"Video ID: {video['videoID']}, Category: {video['category']}, Length: {video['length']}\")\n",
    "\n",
    "get_videos_in_length_range(t1, t2)\n",
    "\n",
    "def get_videos_in_length_range_gui(collection, t1, t2):\n",
    "    general_query = {\n",
    " \"length\": {\"$gte\": t1, \"$lte\": t2}\n",
    "}\n",
    "    result = collection.find(general_query)\n",
    "    return list(result)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d9a054d7c068bdd",
   "metadata": {},
   "source": [
    "### GUI and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "id": "79a84bec14d51b44",
   "metadata": {},
   "source": [
    "# Run tkinter GUI by using 'python GUI.py' command in terminal\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from pymongo import MongoClient\n",
    "import time\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "\n",
    "# MongoDB connection setup\n",
    "uri = 'mongodb://localhost:27017/'\n",
    "client = MongoClient(uri)\n",
    "db = client[\"youtubedb\"]\n",
    "collection = db[\"youtube_vids\"]\n",
    "\n",
    "pagerank_collection = db[\"pagerank_results\"]\n",
    "\n",
    "# Helper function to measure execution time and display results\n",
    "def execute_and_time(func, *args, description=\"\"):\n",
    "    start_time = time.time()\n",
    "    result = list(func(*args))\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return result, execution_time\n",
    "\n",
    "# Get page rank data\n",
    "def get_pagerank(collection):\n",
    "    top_vids = list(collection.find().sort(\"pagerank\", -1).limit(20))\n",
    "    return top_vids\n",
    "\n",
    "# Clear existing plot\n",
    "def clear_plot_area(plot_frame):\n",
    "    for widget in plot_frame.winfo_children():\n",
    "        widget.destroy()\n",
    "\n",
    "# Plot PageRank & category\n",
    "def plot_pagerank(top_vids, plot_frame):\n",
    "\n",
    "    clear_plot_area(plot_frame)\n",
    "    # Get data for visualization\n",
    "    top_categories = [vid[\"category\"] for vid in top_vids]\n",
    "    top_scores = [vid[\"pagerank\"] for vid in top_vids]\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "    # Top 20 videos\n",
    "    axes.bar(top_categories, top_scores, color='green')\n",
    "    axes.set_title(\"Top 20 Videos Category by PageRank\")\n",
    "    axes.set_ylabel(\"PageRank Score\")\n",
    "    axes.set_xlabel(\"Category\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    # Display the plot in the Tkinter GUI\n",
    "    canvas = FigureCanvasTkAgg(fig, master=plot_frame)\n",
    "    canvas.get_tk_widget().pack(pady=10)\n",
    "\n",
    "# Get all PageRank data\n",
    "def get_pagerank_all(collection):\n",
    "    all_vids = list(collection.find())\n",
    "    return all_vids\n",
    "\n",
    "# Plot PageRank vs Other attributes\n",
    "def scatter_plot_pagerank(all_vids, plot_frame, x_key, x_label, plot_title):\n",
    "    clear_plot_area(plot_frame)\n",
    "    pagerank_scores = [vid[\"pagerank\"] for vid in all_vids]\n",
    "    x_data = [vid[x_key] for vid in all_vids]\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(11, 4))\n",
    "\n",
    "    # Scatter plot\n",
    "    axes.scatter(x_data, pagerank_scores, color='blue')\n",
    "    axes.set_title(plot_title)\n",
    "    axes.set_xlabel(x_label)\n",
    "    axes.set_ylabel(\"PageRank Score\")\n",
    "    # x ticks for depth\n",
    "    if x_key == \"depth\":\n",
    "        axes.set_xticks([0, 1, 2, 3])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the plot in the Tkinter GUI\n",
    "    canvas = FigureCanvasTkAgg(fig, master=plot_frame)\n",
    "    canvas.get_tk_widget().pack(pady=10)\n",
    "\n",
    "\n",
    "# Main GUI window\n",
    "class YouTubeAnalyzerApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"YouTube Analyzer\")\n",
    "        self.root.geometry(\"1000x800\")\n",
    "\n",
    "        # Title label\n",
    "        tk.Label(root, text=\"YouTube Analyzer\", font=(\"Arial\", 20)).pack(pady=10)\n",
    "\n",
    "        # Dropdown for selecting analysis option\n",
    "        self.analysis_var = tk.StringVar()\n",
    "        self.analysis_options = [\n",
    "            \"Total View Statistics\",\n",
    "            \"Average Rating by Category\",\n",
    "            \"Total Comments and Ratings by Category\",\n",
    "            \"Average Video Length by Category\",\n",
    "            \"Average Views per Video by Category\",\n",
    "            \"Most Viewed Videos\",\n",
    "            \"Most Viewed Video in Each Category\",\n",
    "            \"Top Uploader by Views\",\n",
    "            \"Top Commented Videos\",\n",
    "            \"Top Video Category by PageRank\",\n",
    "            \"PageRank vs Views analysis\",\n",
    "            \"PageRank vs Rating analysis\",\n",
    "            \"PageRank vs Comments analysis\",\n",
    "            \"PageRank vs Depth analysis\",\n",
    "            \"Top k Categories\",\n",
    "            \"Top k Rated Videos\",\n",
    "            \"Top k Viewed Videos\",\n",
    "            \"All Videos with Length in Range\",\n",
    "            \"Videos in Length Range by Category\"\n",
    "        ]\n",
    "        tk.Label(root, text=\"Select Analysis:\").pack()\n",
    "        self.analysis_menu = ttk.Combobox(\n",
    "            root,\n",
    "            textvariable=self.analysis_var,\n",
    "            values=self.analysis_options,\n",
    "            width=50\n",
    "        )\n",
    "        self.analysis_menu.pack(pady=5)\n",
    "\n",
    "          # Entry for 'k' value \n",
    "        self.k_label = tk.Label(root, text=\"Enter value for k:\")\n",
    "        self.k_label.pack()\n",
    "        self.k_entry = tk.Entry(root)\n",
    "        self.k_entry.pack()\n",
    "        \n",
    "        # Entries for start and end lengths\n",
    "        self.min_length_label = tk.Label(root, text=\"Enter start length (seconds):\")\n",
    "        self.min_length_label.pack()\n",
    "        self.min_length_entry = tk.Entry(root)\n",
    "        self.min_length_entry.pack()\n",
    "\n",
    "        self.max_length_label = tk.Label(root, text=\"Enter end length (seconds):\")\n",
    "        self.max_length_label.pack()\n",
    "        self.max_length_entry = tk.Entry(root)\n",
    "        self.max_length_entry.pack()\n",
    "\n",
    "        self.category_var = tk.StringVar()\n",
    "        self.category_options = [\n",
    "            \"Autos & Vehicles\",\n",
    "            \"Comedy\",\n",
    "            \"Education\",\n",
    "            \"Entertainment\",\n",
    "            \"Film & Animation\",\n",
    "            \"Howto & Style\",\n",
    "            \"Music\",\n",
    "            \"News & Politics\",\n",
    "            \"Nonprofits & Activism\",\n",
    "            \"People & Blogs\",\n",
    "            \"Pets & Animals\",\n",
    "            \"Science & Technology\",\n",
    "            \"Sports\",\n",
    "            \"Travel & Events\",\n",
    "            \"UNA\"\n",
    "        ]\n",
    "        tk.Label(root, text=\"Select Category:\").pack()\n",
    "        self.analysis_menu = ttk.Combobox(\n",
    "            root,\n",
    "            textvariable=self.category_var,\n",
    "            values=self.category_options,\n",
    "            width=50\n",
    "        )\n",
    "        self.analysis_menu.pack(pady=5)\n",
    "        # Button to execute analysis\n",
    "        self.run_button = tk.Button(root, text=\"Run Analysis\", command=self.run_analysis)\n",
    "        self.run_button.pack(pady=10)\n",
    "\n",
    "        # Text area for displaying results\n",
    "        self.result_area = tk.Text(root, wrap=tk.WORD, height=20, width=70)\n",
    "        self.result_area.pack(pady=10)\n",
    "\n",
    "        # Canvas for Matplotlib plots\n",
    "        self.plot_frame = tk.Frame(root)\n",
    "        self.plot_frame.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "    def run_analysis(self):\n",
    "        analysis = self.analysis_var.get()\n",
    "        if not analysis:\n",
    "            messagebox.showerror(\"Error\", \"Please select an analysis.\")\n",
    "            return\n",
    "\n",
    "        self.result_area.delete(\"1.0\", tk.END)\n",
    "\n",
    "        try:\n",
    "            if analysis == \"Total View Statistics\":\n",
    "                stats, exec_time = execute_and_time(calculate_view_statistics, collection, description=\"Platform-Wide View Statistics\")\n",
    "                stats = stats[0]\n",
    "                self.result_area.insert(tk.END, f\"Average Views: {stats['avgViews']:.2f}\\n\")\n",
    "                self.result_area.insert(tk.END, f\"Maximum Views: {stats['maxViews']}\\n\")\n",
    "                self.result_area.insert(tk.END, f\"Minimum Views: {stats['minViews']}\\n\")\n",
    "            elif analysis == \"Average Rating by Category\":\n",
    "                results, exec_time = execute_and_time(calculate_avg_rating_by_category, collection)\n",
    "                for res in results:\n",
    "                    self.result_area.insert(tk.END, f\"Category: {res['_id']}, Avg Rating: {res['avgRating']:.2f}\\n\")\n",
    "            elif analysis == \"Total Comments and Ratings by Category\":\n",
    "                results, exec_time = execute_and_time(calculate_total_comments_and_ratings_by_category, collection)\n",
    "                for res in results:\n",
    "                    self.result_area.insert(\n",
    "                        tk.END,\n",
    "                        f\"Category: {res['_id']}, Total Comments: {res['totalComments']}, Total Ratings: {res['totalRatings']}\\n\"\n",
    "                    )\n",
    "            elif analysis == \"Average Video Length by Category\":\n",
    "                results, exec_time = execute_and_time(calculate_avg_length_by_category, collection)\n",
    "                for res in results:\n",
    "                    self.result_area.insert(tk.END, f\"Category: {res['_id']}, Avg Length: {res['avgLength']:.2f}\\n\")\n",
    "            elif analysis == \"Average Views per Video by Category\":\n",
    "                results, exec_time = execute_and_time(avg_views_per_video_by_category, collection)\n",
    "                for res in results:\n",
    "                    self.result_area.insert(tk.END, f\"Category: {res['_id']}, Avg Views: {res['avgViews']:.2f}\\n\")\n",
    "            elif analysis == \"Most Viewed Videos\":\n",
    "                results, exec_time = execute_and_time(most_viewed_videos, collection, 5)\n",
    "                for res in results:\n",
    "                    self.result_area.insert(tk.END, f\"Video ID: {res['videoID']}, Views: {res['views']}\\n\")\n",
    "            elif analysis == \"Most Viewed Video in Each Category\":\n",
    "                results, exec_time = execute_and_time(most_viewed_video_in_each_category, collection)\n",
    "                for res in results:\n",
    "                    self.result_area.insert(\n",
    "                        tk.END,\n",
    "                        f\"Category: {res['_id']}, Video ID: {res['videoID']}, Views: {res['views']}\\n\"\n",
    "                    )\n",
    "            elif analysis == \"Top Uploader by Views\":\n",
    "                result, exec_time = execute_and_time(top_uploader_by_views, collection)\n",
    "                result = result[0]\n",
    "                self.result_area.insert(tk.END, f\"Uploader: {result['_id']}, Total Views: {result['totalViews']}\\n\")\n",
    "            elif analysis == \"Top Commented Videos\":\n",
    "                results, exec_time = execute_and_time(top_commented_videos, collection)\n",
    "                for res in results:\n",
    "                    self.result_area.insert(\n",
    "                        tk.END,\n",
    "                        f\"Video ID: {res['videoID']}, Comments: {res['comments']}, Uploader: {res['uploader']}\\n\"\n",
    "                    )\n",
    "            elif analysis == \"Top Video Category by PageRank\":\n",
    "                top_videos, exec_time = execute_and_time(get_pagerank, pagerank_collection)\n",
    "\n",
    "                # Show results in text area\n",
    "                self.result_area.insert(tk.END, \"Top Videos Category by PageRank:\\n\")\n",
    "                for video in top_videos:\n",
    "                    self.result_area.insert(\n",
    "                        tk.END,\n",
    "                        f\"Video ID: {video['id']}, PageRank: {video['pagerank']:.4f}, Category: {video['category']}\\n\"\n",
    "                    )\n",
    "                plot_pagerank(top_videos, self.plot_frame)\n",
    "\n",
    "            elif analysis == \"PageRank vs Views analysis\":\n",
    "                top_videos, exec_time = execute_and_time(get_pagerank, pagerank_collection)\n",
    "\n",
    "                # Show only the results for the top 20 videos in text area\n",
    "                self.result_area.insert(tk.END, \"PageRank vs. Views:\\n\")\n",
    "                for video in top_videos:\n",
    "                    self.result_area.insert(\n",
    "                        tk.END,\n",
    "                        f\"Video ID: {video['id']}, PageRank: {video['pagerank']:.4f}, Views: {video['views']}\\n\"\n",
    "                    )\n",
    "                all_videos = get_pagerank_all(pagerank_collection)\n",
    "                # Plot the visualization for all 647k videos (pagerank score & number of views)\n",
    "                scatter_plot_pagerank(all_videos, self.plot_frame, \"views\", \"Number of views\", \"PageRank vs Views\")\n",
    "\n",
    "            elif analysis == \"PageRank vs Rating analysis\":\n",
    "                top_videos, exec_time = execute_and_time(get_pagerank, pagerank_collection)\n",
    "\n",
    "                # Show only the results for the top 20 videos in text area\n",
    "                self.result_area.insert(tk.END, \"PageRank vs. Rate:\\n\")\n",
    "                for video in top_videos:\n",
    "                    self.result_area.insert(\n",
    "                        tk.END,\n",
    "                        f\"Video ID: {video['id']}, PageRank: {video['pagerank']:.4f}, Rate: {video['rate']}\\n\"\n",
    "                    )\n",
    "                all_videos = get_pagerank_all(pagerank_collection)\n",
    "                # Plot the visualization for all 647k videos (pagerank & rating score)\n",
    "                scatter_plot_pagerank(all_videos, self.plot_frame, \"rate\", \"Rating\", \"PageRank vs Video Rating\")\n",
    "\n",
    "            elif analysis == \"PageRank vs Comments analysis\":\n",
    "                top_videos, exec_time = execute_and_time(get_pagerank, pagerank_collection)\n",
    "\n",
    "                # Show only the results for the top 20 videos in text area\n",
    "                self.result_area.insert(tk.END, \"PageRank vs. Comments:\\n\")\n",
    "                for video in top_videos:\n",
    "                    self.result_area.insert(\n",
    "                        tk.END,\n",
    "                        f\"Video ID: {video['id']}, PageRank: {video['pagerank']:.4f}, Comment: {video['comments']}\\n\"\n",
    "                    )\n",
    "                all_videos = get_pagerank_all(pagerank_collection)\n",
    "                # Plot the visualization for all 647k videos (pagerank score & number of comments)\n",
    "                scatter_plot_pagerank(all_videos, self.plot_frame, \"comments\", \"Number of comments\", \"PageRank vs Comments\")\n",
    "\n",
    "            elif analysis == \"PageRank vs Depth analysis\":\n",
    "                top_videos, exec_time = execute_and_time(get_pagerank, pagerank_collection)\n",
    "\n",
    "                # Show only the results for the top 20 videos in text area\n",
    "                self.result_area.insert(tk.END, \"PageRank vs. Depths:\\n\")\n",
    "                for video in top_videos:\n",
    "                    self.result_area.insert(\n",
    "                        tk.END,\n",
    "                        f\"Video ID: {video['id']}, PageRank: {video['pagerank']:.4f}, Depth: {video['depth']}\\n\"\n",
    "                    )\n",
    "                all_videos = get_pagerank_all(pagerank_collection)\n",
    "                # Plot the visualization for all 647k videos (pagerank score & depth level)\n",
    "                scatter_plot_pagerank(all_videos, self.plot_frame, \"depth\", \"Depth Level\", \"PageRank vs Depth\")\n",
    "            elif analysis == \"Top k Categories\":\n",
    "                if self.k_entry.get():\n",
    "                    k = int(self.k_entry.get())\n",
    "                else:\n",
    "                    messagebox.showerror(\"Error\", \"Please enter a value for k.\")\n",
    "                    return\n",
    "                results, exec_time = execute_and_time(top_k_categories_gui, collection, k)\n",
    "                self.result_area.insert(tk.END, \"Top k Categories:\\n\")\n",
    "                for res in results:\n",
    "                    self.result_area.insert(tk.END, f\"Category: {res['_id']}, Video Count: {res['count']:.2f}\\n\")\n",
    "            elif analysis == \"Top k Rated Videos\":\n",
    "                if self.k_entry.get():\n",
    "                    k = int(self.k_entry.get())\n",
    "                else:\n",
    "                    messagebox.showerror(\"Error\", \"Please enter a value for k.\")\n",
    "                    return\n",
    "                results, exec_time = execute_and_time(top_k_rated_videos_gui, collection, k)\n",
    "                self.result_area.insert(tk.END, f\"Top {k} Rated Videos:\\n\")\n",
    "                for res in results:\n",
    "                    self.result_area.insert(tk.END, f\"Video: {res['_id']}, Rating: {res['rate']:.2f}\\n\")\n",
    "            elif analysis == \"Top k Viewed Videos\":\n",
    "                if self.k_entry.get():\n",
    "                    k = int(self.k_entry.get())\n",
    "                else:\n",
    "                    messagebox.showerror(\"Error\", \"Please enter a value for k.\")\n",
    "                    return\n",
    "                results, exec_time = execute_and_time(top_k_viewed_videos_gui, collection, k)\n",
    "                self.result_area.insert(tk.END, f\"Top {k} Viewed Videos\\n\")\n",
    "                for res in results:\n",
    "                    self.result_area.insert(tk.END, f\"Category: {res['_id']}, View Count: {res['views']:.2f}\\n\")\n",
    "            elif analysis == \"All Videos with Length in Range\":\n",
    "                if self.min_length_entry.get() and self.max_length_entry.get():\n",
    "                    start = int(self.min_length_entry.get())\n",
    "                    end = int(self.max_length_entry.get())\n",
    "                else:\n",
    "                    messagebox.showerror(\"Error\", \"Please enter a min and max length.\")\n",
    "                    return\n",
    "                results, exec_time = execute_and_time(get_videos_in_length_range_gui, collection, start, end)\n",
    "                self.result_area.insert(tk.END, f\"Videos with Length in Range {start} to {end} seconds:\\n\")\n",
    "                for res in results:\n",
    "                    self.result_area.insert(tk.END, f\"Video: {res['_id']}, Length: {res['length']:.2f}\\n\")\n",
    "            elif analysis == \"Videos in Length Range by Category\":\n",
    "                if self.min_length_entry.get() and self.max_length_entry.get():\n",
    "                    start = int(self.min_length_entry.get())\n",
    "                    end = int(self.max_length_entry.get())\n",
    "                else:\n",
    "                    messagebox.showerror(\"Error\", \"Please enter a min and max length.\")\n",
    "                    return                \n",
    "                if self.category_var.get():\n",
    "                    category = self.category_var.get()\n",
    "                else:\n",
    "                    messagebox.showerror(\"Error\", \"Please select a category.\")\n",
    "                    return              \n",
    "                results, exec_time = execute_and_time(videos_in_length_range_by_category_gui, collection, category, start, end)\n",
    "                self.result_area.insert(tk.END, f\"Videos in category {category} with Length in Range {start} to {end} seconds:\\n\")\n",
    "                for res in results:\n",
    "                    self.result_area.insert(tk.END, f\"Video: {res['_id']}, Length: {res['length']:.2f}\\n\")\n",
    "            # Show execution time\n",
    "            self.result_area.insert(tk.END, f\"\\nExecution Time: {exec_time:.4f} seconds\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Run the application\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = YouTubeAnalyzerApp(root)\n",
    "    root.mainloop()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
